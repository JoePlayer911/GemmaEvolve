# gemma_config.yaml
# Complete configuration for running OpenEvolve with local Gemma

# LLM Configuration
llm:
  models:
    - name: "gemma-local"
      model_path: "/home/jonathan13/GemmaEvolve/gemma-3-12b-it-Q8_0.gguf"
      
      # GGUF Specific Settings
      n_ctx: 8192            # Context window size
      n_gpu_layers: -1       # -1 = Offload all layers to GPU
      
      # Generation Parameters
      temperature: 0.7
      top_p: 0.95
      max_tokens: 4196
      
      # System Prompt
      system_message: |
        You are an expert Python programmer. Write high-quality, efficient code.
        Always provide the complete code in a single block.

  # Use the same model for evaluation
  evaluator_models:
    - name: "gemma-local" 
      model_path: "/home/jonathan13/GemmaEvolve/gemma-3-12b-it-Q8_0.gguf"
      n_ctx: 8192
      n_gpu_layers: -1
      
# General Settings
max_iterations: 10
# Early Stopping
# target_score: 0.95        # Stop if this score is reached (optional)
# early_stopping_patience: 5 # Stop if no improvement after N iterations (optional)
# convergence_threshold: 0.001 # Minimum improvement to reset patience
# early_stopping_metric: "combined_score" # Metric to track for improvement
language: python
file_suffix: .py
project_name: "gemma_test"

# Database & Evolution Settings
database:
  population_size: 10 # Smaller population for testing, might overload VRAM if too large
  num_islands: 1
  programs_per_island: 10

evaluator:
  timeout: 30
  parallel_evaluations: 2 # Use 4 concurrent workers (one per GPU)