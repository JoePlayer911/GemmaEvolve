# gemma_config.yaml
# Complete configuration for running OpenEvolve with local Gemma

# LLM Configuration
llm:
  models:
    - name: "gemma-local"
      model_path: "/home/jonathan13/GemmaEvolve/gemma-3-12b-it-Q8_0.gguf"
      
      # GGUF Specific Settings
      n_ctx: 8192            # Context window size
      n_gpu_layers: -1       # -1 = Offload all layers to GPU
      
      # Generation Parameters
      temperature: 0.7
      top_p: 0.95
      max_tokens: 4196
      
      # System Prompt
      system_message: |
        You are an expert Python programmer. Write high-quality, efficient code.
        Always provide the complete code in a single block.

  # Use the same model for evaluation
  evaluator_models:
    - name: "gemma-local" 
      model_path: "/home/jonathan13/GemmaEvolve/gemma-3-12b-it-Q8_0.gguf"
      n_ctx: 8192
      n_gpu_layers: -1
      
# General Settings
max_iterations: 10
language: python
file_suffix: .py
project_name: "gemma_test"

# Database & Evolution Settings
database:
  population_size: 10 # Smaller population for testing, might overload VRAM if too large
  num_islands: 1
  programs_per_island: 10

evaluator:
  timeout: 30
  parallel_evaluations: 2 # Use 4 concurrent workers (one per GPU)