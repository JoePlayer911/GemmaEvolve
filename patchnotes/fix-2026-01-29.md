# Fix 2026-01-29

## Verilog Benchmark Suite
We have introduced a comprehensive benchmarking tool `benchmark_verilog.py` designed to rigorously compare the performance of raw LLM generation against our evolutionary optimization engine.

This suite automates the process of:
1.  **Baseline Testing:** Running a zero-shot inference with the local Gemma model ("Pure Gemma").
2.  **Evolutionary Testing:** Running the full OpenEvolve loop with Early Stopping.
3.  **Analysis:** Recording execution time and final accuracy scores to quantify the benefit of the evolutionary approach.

```python
# Usage
python3 benchmark_verilog.py --limit 10 --patience 5
```

## Insights: Evolution vs Baseline
Early results from the benchmark (running on the `verilog-eval` dataset) highlight the trade-offs:

*   **Pure Gemma**: Extremely fast (seconds), but often fails on complex logic or syntax nuances, resulting in 0.0 accuracy for harder problems.
*   **OpenEvolve**: Slower (minutes), but consistently achieves higher accuracy by iteratively refining the code and fixing syntax errors via the compiler feedback loop.

## Infrastructure Improvements
*   **Agentic Config Repair**: We hardened the `agentic_convert.py` script to use robust YAML dumping libraries instead of string templates. This eliminated a class of "ScannerError" bugs caused by improper indentation of multiline Verilog code in configuration files.
