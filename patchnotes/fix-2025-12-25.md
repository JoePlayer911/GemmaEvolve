# Fix Report - 2025-12-25
## Linux Migration and Local LLM Execution Stability

### 1. The Problem: Multiprocessing Crash
When running the project on Linux with a local Gemma model, the process would crash with:
`ValueError: Failed to create llama_context` or `A process in the process pool was terminated abruptly`.

*   **Cause**: The `multiprocessing` library's "spawn" method on Linux conflicts with `llama-cpp-python`'s CUDA context management when re-initialized in a child process.
*   **Conflict**: `asyncio.run()` cannot be called from within a running event loop, which occurred when trying to run the worker logic directly in the main thread to avoid spawning.

### 2. The Fix: ThreadPoolExecutor Bypass
Modified the parallel controller to use a hybrid execution model:
*   **Change**: Updated `openevolve/process_parallel.py` to use `ThreadPoolExecutor` instead of `ProcessPoolExecutor` when `parallel_evaluations` is configured to `1`.
*   **Benefit**: This maintains technical isolation for `asyncio` (allowing a new loop in the thread) while staying within the same system process, which prevents CUDA context crashes and significantly reduces memory overhead.

### 3. Changes and Migration
*   **New Scripts**: Created Linux shell scripts (`run_gemma_debug.sh`, `run_gemma_function_minimization.sh`) for environment compatibility.
*   **Path Updates**: Updated `gemma_config.yaml` to use Linux-style absolute paths and fixed GPU layer settings.
*   **Execution Bits**: Applied `chmod +x` to all script files.

### 4. Verification
Confirmed via `nvidia-smi` and debug logs that:
*   Model loads successfully across available GPUs (4x 1080Ti).
*   The evolution loop proceeds without "abrupt termination" errors.
*   GPU memory utilization is stable.

### 5. Context Window Limit Fix
- **Problem**: Long prompts (~5500 tokens) combined with generation requests were causing `Requested tokens exceed context window` errors with the default 4096 limit.
- **Fix**: Increased `n_ctx` from 4096 to **16384** in `examples/function_minimization/gemma_config.yaml`.
- **Result**: Evolution iterations now complete successfully (verified active generation with ~16GB memory usage).
