# Fix 2025-12-25

## Linux Migration & Stability
Moving from development to a Linux production environment revealed a major instability with Python's `multiprocessing`. When Python spawns a new process on Linux, it re-initializes the CUDA context in a way that conflicts with `llama-cpp-python` if done naively, leading to immediate crashes.

`ValueError: Failed to create llama_context`

## The ThreadPool Fix
To stabilize this, we implemented a hybrid execution model. When running single-stream evaluations (Serial Mode), we bypass the heavy `ProcessPoolExecutor` entirely and use a `ThreadPoolExecutor`.

This keeps everything in the same system process. It avoids the overhead of spawning a new shell and re-loading the massive libraries, preventing the context conflict. It essentially "cheats" the system constraints to allow reliable local execution for testing and debugging.

```python
# Hybrid logic in process_parallel.py
if parallel_evaluations == 1:
    executor = ThreadPoolExecutor() # Fast, safe for single stream
else:
    executor = ProcessPoolExecutor() # robust isolation for heavy load
```
