# Fix 2026-01-07

## Multi-GPU VRAM Logic
We faced a cascading failure when trying to run the extensive **Gemma 12B Q8** model on our 1080 Ti hardware. The system would crash with an obscure `AttributeError`, but the root cause was simple physics: 12.5GB of model does not fit into 11GB of VRAM.

When the load failed, the library tried to clean up an object that was never fully created, leading to the confusing error message.

## Dynamic GPU Distribution
The solution was to rewrite how we assign GPUs to workers. Instead of giving each worker 1 GPU (which meant 11GB VRAM limit), we implemented a dynamic splitter. The controller now counts available GPUs and divides them up.

If we have 4 GPUs and 2 Workers, the math changes automatically so each worker gets 2 GPUs. This gives each worker 22GB of combined VRAMâ€”plenty of room for the high-quality Q8 model.

```python
# process_parallel.py logic
# From: gpu_id = 0
# To:   gpu_str = "0,1" 
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
```

This transparency allows the underlying engine to "see" multiple cards and split the model layers across them automatically, unlocking high-precision evolution on older hardware.
