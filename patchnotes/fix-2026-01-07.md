# Patch Notes: 2026-01-07 - Multi-GPU VRAM Logic Fix

## Issue Summary
The system was encountering a confusing crash during the evolution process:
`AttributeError: 'LlamaModel' object has no attribute 'sampler'`

### The Root Cause
The 12.5GB **Q8 model** (`gemma-3-12b-it-Q8_0.gguf`) was too large for the 11GB VRAM of a single **GTX 1080 Ti**. 

1. When `parallel_evaluations` was set to 2, the system tried to load the model twice (once for each worker).
2. Each worker was pinned to a single GPU.
3. Since 12.5GB > 11GB, the model load failed with a `ValueError`.
4. The `AttributeError` was a **cascading failure**: `llama-cpp-python` tried to clean up the failed model object by closing its "sampler", but because the model never finished loading, the sampler attribute didn't exist yet.

## What Changed?

We modified the **GPU Management System** in `openevolve/process_parallel.py` to support **Multi-GPU Splitting**.

### 1. Dynamic GPU Distribution
Previously, the controller assigned exactly one GPU index to each worker. Now, it calculates how many GPUs each worker can have based on the available hardware.

**Code Change in `ProcessParallelController`:**
```python
# Assign GPUs to workers
# We divide available GPUs among workers (e.g., 4 GPUs / 2 Workers = 2 GPUs per worker)
gpus_per_worker = max(1, len(available_gpus) // max(1, self.num_workers))

for i in range(self.num_workers):
    worker_gpus = []
    for j in range(gpus_per_worker):
        # Group GPUs sequentially for each worker
        gpu_idx = (i * gpus_per_worker + j) % len(available_gpus)
        worker_gpus.append(available_gpus[gpu_idx])
    
    # Create comma-separated string (e.g., "0,1" or "2,3")
    gpu_str = ",".join(map(str, worker_gpus))
    self.gpu_queue.put(gpu_str)
```

### 2. Worker Transparency
The worker process was updated to handle these multi-GPU strings. By setting `CUDA_VISIBLE_DEVICES` to a list like `"0,1"`, the underlying `llama-cpp` engine automatically detects both cards and splits the model layers across them.

**Code Change in `_worker_init`:**
```python
if gpu_queue:
    try:
        gpu_id = gpu_queue.get(timeout=1.0) # Gets "0,1" instead of just "0"
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        print(f"Worker process {os.getpid()} pinned to GPU {gpu_id}")
```

## Why It Matters

### 1. High-Precision Evolution
By splitting the model across 2 GPUs (total 22GB VRAM per worker), we can use the **Q8 model** instead of being forced to downgrade to a lower-quality Q4 quantization. This results in significantly better code generation quality during the evolution process.

### 2. Scalability
The logic is now dynamic. If you add more GPUs or change the parallelism level, the system will automatically re-calculate the densest possible distribution:
- **2 Workers + 4 GPUs**: Each worker gets 2 GPUs (Current Setup).
- **4 Workers + 4 GPUs**: Each worker gets 1 GPU (Requires smaller model).
- **1 Worker + 4 GPUs**: Worker gets all 4 GPUs.

### 3. Stability
This fix addresses the obscure `AttributeError` by ensuring the model actually has enough memory to boot, preventing the faulty cleanup path from ever being triggered.
