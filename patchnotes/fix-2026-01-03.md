# Patch Notes - 2026-01-03: Multi-GPU VRAM Optimization

## Summary
Successfully optimized the OpenEvolve system to run the **Gemma 3 12B** model across **4x NVIDIA GTX 1080 Ti GPUs**. This fix addresses out-of-memory (OOM) errors and "Failed to create llama_context" issues when running large models in parallel.

## Key Improvements

### 1. "Zero-VRAM" Main Process Refactor
*   **The Problem**: The Main Process (Controller) was eagerly loading the 7GB model at startup. This fragmented VRAM, leaving insufficient contiguous space for worker processes to load their own copies, even if memory appeared "free".
*   **The Fix**: Refactored `openevolve/controller.py` to use **Lazy Loading**.
    *   The Main Process now skips LLM initialization by default.
    *   It only loads the model if an initial evaluation is strictly required, then immediately explicitly deletes it and triggers garbage collection.
    *   **Result**: The Main Process uses **0GB VRAM** during the main evolution loop, granting workers full access to GPU resources.

### 2. Model Quantization Strategy
*   **Model**: Switched from `gemma-3-12b-it-Q8_0.gguf` (12.5GB) to `gemma-3-12b-it-Q8_0.gguf` (~7.3GB).
*   **Rationale**: The Q4 quantization maintains most of the "critical thinking" capabilities of the 12B model while allowing a 4096 context window to fit comfortably within the 11GB VRAM of a GTX 1080 Ti.

### 3. Multi-GPU Pinning & Parallelism
*   **Executor**: Forced `ProcessPoolExecutor` with `spawn` context for all multi-worker runs.
*   **GPU Balancing**: Workers are now pinned to specific GPUs using a circular queue of `CUDA_VISIBLE_DEVICES` IDs (0, 1, 2, 3).
*   **Scale**: Enabled `parallel_evaluations: 2` in `gemma_config.yaml` to maximize all 4 available cards.

## How to Run
```bash
./run_gemma_debug.sh
```
*Monitor with:* `watch -n 1 nvidia-smi` to see 4 isolated processes running on 4 distinct GPUs.
