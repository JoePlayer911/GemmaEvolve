# Fix 2026-01-28

## Agentic Dataset Conversion

We have introduced a new automated pipeline that capability of our local Gemma model. Rather than relying on rigid, hardcoded rules to convert the `verilog-eval` dataset, we created an autonomous agent that "reads" the problems and writes the necessary benchmark scaffolding itself.

The core of this logic lies in `agentic_convert.py`. This script iterates through raw problem files and asks the local LLM to understand the circuit requirements. Instead of us manually writing regex parsers for 156 different testbenches, the model generates a custom `evaluator.py` for each unique problem structure.

```python
# The agent dynamically generates python code based on the specific testbench it reads
evaluator_code = generate_evaluator(llm, testbench_snippet)
```

## Insight: One-Shot Prompting
During development, we discovered that while the model is intelligent, it can be "creative" in dangerous ways. Initial runs saw the model inventing its own scoring metricsâ€”often rewarding longer code merely because it had more lines. To correct this without writing complex constraints, we applied One-Shot Prompting.

By injecting a single, perfect example of an evaluator into the system prompt, we aligned the model's output immediately. The model stopped hallucinating unsafe metrics and started adhering to strict safety standards (like using `tempfile`) while retaining the flexibility to parse novel text output.

```python
# We teach the model by example, not just instruction
one_shot_example = """
def evaluate(code):
    ...
    # Correct scoring: Accuracy is paramount
    if accuracy == 1.0:
        score += bonus
"""
```

This confirmed that for code generation tasks, a single concrete example is often worth a page of instructions.
